= Predicting Source Code Readability

This Python program preprocesses Java source code snippets for predicting their readability.
The model itself is not part of this repository.

== Table of Contents

* <<Installation>>
* <<Usage>>
** <<Extract_Files>>
** <<Extract_Methods>>
** <<Convert_datasets>>
*** <<Convert_datasets_csv>>
*** <<Convert_datasets_two_folders>>
** <<Combine_datasets>>
** <<Download_datasets>>
** <<Upload_datasets>>
** <<Sample>>
** <<Preprocess>>
*** <<Visualize>>

[[Installation]]
== Installation

To set up the project and its dependencies, follow these steps:

Clone this repository to your local machine:

[source,bash]
----
git clone https://github.com/LuKrO2011/readability-preprocessing
cd readability-preprocessing
----

Install Poetry if you haven't already:

[source,bash]
----
pip install poetry
----

Create a virtual environment and install the project's dependencies using Poetry:

[source,bash]
----
poetry install
----

Activate the virtual environment:

[source,bash]
----
poetry shell
----

Now you're ready to use the source code readability preprocessing tool.

[[Usage]]
== Usage

[[Extract_Files]]
=== Extract Files

To extract Java source code files from multiple directories, use the following command:

[source,bash]
----
main.py EXTRACT_FILES [-h] --input INPUT --output OUTPUT [--non-violated-subdir NON_VIOLATED_SUBDIR]
----

* `--input` or `-i` is the path to the directory containing the directories with the Java source code files.
* `--output` or `-o` is the path to the directory where the extracted Java source code files will be saved.
* `--non-violated-subdir` or `-nvs` is the name of the subdirectory where the non-violated Java source code files are saved.

Example:

[source,bash]
----
python src/readability_preprocessing/main.py EXTRACT_FILES -i <input_path> -o <output_path>
----

[[Extract_Methods]]
=== Extract Methods

To extract methods from a dictionary of Java source code snippets, use the following command:

[source,bash]
----
python src/readability_preprocessing/main.py EXTRACT_METHODS --input INPUT --output OUTPUT [--overwrite-mode {OverwriteMode.OVERWRITE,OverwriteMode.SKIP}] [--include-method-comments INCLUDE_METHOD_COMMENTS] [--comments-required COMMENTS_REQUIRED] [--remove-indentation REMOVE_INDENTATION]
----

* `--input` or `-i` is the path to the dictionary with the Java source code snippets.
* `--output` or `-o` is the path to the directory where the extracted methods will be saved.
* `--overwrite-mode` or `-om` is the overwrite mode to use.
The default is `OverwriteMode.SKIP`.
* `--include-method-comments` or `-imc` is a boolean flag indicating whether to include method comments in the extracted methods.
The default is `True`.
* `--comments-required` or `-cr` is a boolean flag indicating whether to require comments for extracted methods.
The default is `True`.
* `--remove-indentation` or `-ri` is a boolean flag indicating whether to remove indentation from the extracted methods.
The default is `True`.

Example:

[source,bash]
----
python src/readability_preprocessing/main.py EXTRACT_METHODS -i <input_path> -o <output_path>
----

[[Convert_datasets]]
=== Convert datasets

This tool supports converting datasets from csv and a dictionary of Java source code snippets to a HuggingFace dataset.
To do this, see <<Convert_datasets_csv>>.
You can also convert two folders containing Java source code files, one folder with readable and the other with non-readable Java source code files, to a HuggingFace dataset.
To do this, see <<Convert_datasets_two_folders>>.

[[Convert_datasets_csv]]
[[Convert_datasets_csv]]
==== Convert datasets csv

To convert a csv file to a HuggingFace dataset, use the following command:

[source,bash]
----
src/readability_preprocessing/main.py CONVERT_CSV [-h] --input INPUT --csv CSV --output OUTPUT --dataset-type {SCALABRIO,BW,DORN}
----

* `--input` or `-i` is the path to the directory containing the directories with the Java source code files.
* `--csv` or `-c` is the path to the csv file containing the paths and features of the java files.
* `--output` or `-o` is the path to the directory where the converted dataset will be saved.
* `--dataset-type` or `-dt` is the type of the dataset to convert.
Currently, the following types are supported: `SCALABRIO`, `BW`, `DORN`.

Example:

[source,bash]
----
python src/readability_preprocessing/main.py CONVERT_CSV -i <input_path> -c <csv_path> -o <output_path> -dt SCALABRIO
----

[[Convert_datasets_two_folders]]
==== Convert datasets from two folders

To convert two folders containing Java source code files, one folder with readable and the other with non-readable Java source code files, to a HuggingFace dataset, use the following command:

[source,bash]
----
python src/readability_preprocessing/main.py CONVERT_TWO_FOLDERS [-h] --readable READABLE --not-readable NOT_READABLE --output OUTPUT [--readable-score READABLE_SCORE] [--not-readable-score NOT_READABLE_SCORE]
----

* `--readable` or `-r` is the path to the directory containing the readable Java source code files.
* `--not-readable` or `-nr` is the path to the directory containing the non-readable Java source code files.
* `--output` or `-o` is the path to the directory where the converted dataset will be saved.
* `--readable-score` or `-rs` is the score to assign to the readable Java source code files.
The default is `4.5`.
* `--not-readable-score` or `-nrs` is the score to assign to the non-readable Java source code files.
The default is `1.5`.

Example:

[source,bash]
----
python src/readability_preprocessing/main.py CONVERT_TWO_FOLDERS -r <readable_path> -nr <not_readable_path> -o <output_path>
----

[[Combine_datasets]]
=== Combine datasets

To combine multiple HuggingFace datasets into one, use the following command:

[source,bash]
----
python src/readability_preprocessing/main.py COMBINE [-h] --input INPUT [INPUT ...] --output OUTPUT [--percent-to-remove PERCENT_TO_REMOVE]
----

* `--input` or `-i` is the paths to the directories containing the HuggingFace datasets.
* `--output` or `-o` is the path to the directory where the combined dataset will be saved.
* `--percent-to-remove` or `-ptr` is the percentage of examples to remove from the combined dataset.

Example:

[source,bash]
----
python src/readability_preprocessing/main.py COMBINE -i <input_path_1> <input_path_2> -o <output_path>
----

[[Sample]]
=== Sample

To sample from a dictionary of Java source code snippets, use the following command:

[source,bash]
----
python src/readability_preprocessing/main.py SAMPLE --input INPUT [--save SAVE] [--num-stratas NUM_STRATAS] [--snippets-per-stratum SNIPPETS_PER_STRATUM]
----

* `--input` or `-i` is the path to the dictionary with the Java source code snippets or to a csv file containing the paths and features of the java files.
* `--save` or `-s` is the path to the file where the sampled snippets and the features (as csv) will be saved.
* `--num-stratas` or `-n` is the number of stratas to sample from.
* `--snippets-per-stratum` or `-sps` is the number of snippets to sample from each stratum.

Example:

[source,bash]
----
python src/readability_preprocessing/main.py SAMPLE -i <input_path> -s <save_path>
----

[[Download_datasets]]
=== Download datasets

To download a dataset from the HuggingFace Hub, use the following command:

[source,bash]
----
python src/readability_preprocessing/main.py DOWNLOAD [-h] --name NAME --output OUTPUT [--token-file TOKEN_FILE]
----

* `--name` or `-n` is the name of the dataset to download.
* `--output` or `-o` is the path to the directory where the downloaded dataset will be saved.
* `--token-file` or `-tf` is the path to the file containing the HuggingFace API token.
If not provided, the dataset must be public.

Example:

[source,bash]
----
python src/readability_preprocessing/main.py DOWNLOAD -n <dataset_name> -o <output_path>
----

[[Upload_datasets]]
=== Upload datasets

To upload a dataset to the HuggingFace Hub, use the following command:

[source,bash]
----
python src/readability_preprocessing/main.py UPLOAD [-h] --input INPUT --name NAME --token-file TOKEN_FILE
----

* `--input` or `-i` is the path to the directory containing the dataset to upload.
* `--name` or `-n` is the name of the dataset to upload.
* `--token-file` or `-tf` is the path to the file containing the HuggingFace API token.

Example:

[source,bash]
----
python src/readability_preprocessing/main.py UPLOAD -i <input_path> -n <dataset_name> -tf <token_file_path>
----

[[Preprocess]]
=== Preprocess

For preprocessing there are multiple options:

[[Visualize]]
==== Visualize

To visualize the distribution of the features, use the following command:

[source,bash]
----
python src/readability_preprocessing/main.py VISUALIZE --input INPUT --save SAVE [--css CSS] [--width WIDTH] [--height HEIGHT]
----

* `--input` or `-i` is the path to the file containing a java file or a dictionary with java files.
* `--save` or `-s` is the path to the directory where the output images will be saved.
* `--css` or `-c` is the path to the CSS file to use for styling the output images.
* `--width` or `-w` is the width of the output images.
* `--height` or `-h` is the height of the output images.

Example:

[source,bash]
----
python src/readability_preprocessing/main.py VISUALIZE -i <input_path> -s <save_path>
----

